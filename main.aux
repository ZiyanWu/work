\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{elsarticle-num}
\citation{Jiang2017Pytheas:Exploration-Exploitation}
\citation{Mao2017NeuralPensieve}
\citation{Li2017}
\citation{JiangVIA:Selection}
\citation{GaoMachineOptimization}
\citation{Lecun2015}
\@LN@col{1}
\Newlabel{mycorrespondingauthor}{1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@LN@col{2}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The basic working procedure of CDN}}{2}{figure.1}}
\newlabel{fig}{{1}{2}{The basic working procedure of CDN}{figure.1}{}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The figure}}{2}{figure.2}}
\newlabel{fig: the structure of PoP}{{2}{2}{The figure}{figure.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces list of candidate input features from one cache server. We organize the features into groups. The first group is the raw data we directly collected from the caching servers. The second group are the features we construct based on statistics of the metrics of a single cache server. The third group are the features we construct based on the statistics of the metrics of the whole cache server groups}}{3}{table.1}}
\newlabel{my-label}{{1}{3}{list of candidate input features from one cache server. We organize the features into groups. The first group is the raw data we directly collected from the caching servers. The second group are the features we construct based on statistics of the metrics of a single cache server. The third group are the features we construct based on the statistics of the metrics of the whole cache server groups}{table.1}{}}
\@LN@col{2}
\@writefile{toc}{\contentsline {section}{\numberline {3}Problem Formulation and Models}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}linear models for sequence learning}{3}{subsection.3.1}}
\citation{HermansTrainingNetworks}
\citation{Bengio1994LearningDifficult}
\citation{ChoLearningTranslation}
\@LN@col{1}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}non-linear for sequence learning}{4}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Basic feed forward neural network}{4}{subsubsection.3.2.1}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces MLP structure}}{4}{figure.3}}
\newlabel{fig:RNN}{{3}{4}{MLP structure}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}RNN}{4}{subsubsection.3.2.2}}
\citation{pascanu2012difficulty}
\citation{Hochreiter1997LongMemory}
\citation{MalhotraLongSeries}
\citation{ChoLearningTranslation}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The left shows the backloop structure of RNN and right shows that RNN can be thought as infinite deep layers neural network unfolded along the dimension of time}}{5}{figure.4}}
\newlabel{fig:RNN can be thought as infinite deep layers neural network along the dimensions of time}{{4}{5}{The left shows the backloop structure of RNN and right shows that RNN can be thought as infinite deep layers neural network unfolded along the dimension of time}{figure.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}LSTM}{5}{subsubsection.3.2.3}}
\@LN@col{2}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.4}LSTM auto-encoder}{5}{subsubsection.3.2.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces LSTM Auto-encoder Model}}{5}{figure.5}}
\newlabel{fig:RNN_encoder-decoder}{{5}{5}{LSTM Auto-encoder Model}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Methods}{5}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Feature Engineering}{5}{subsection.4.1}}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces three steps of feature enginnering}}{6}{figure.6}}
\newlabel{fig:Correlation_matrix}{{6}{6}{three steps of feature enginnering}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Prediction Model Design}{6}{subsection.4.2}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Correlation Matrix of the feature set}}{6}{figure.7}}
\newlabel{fig:Correlation_matrix}{{7}{6}{Correlation Matrix of the feature set}{figure.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Evaluation}{6}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Experimental Settings and Dataset}{6}{subsection.5.1}}
\citation{Lecun2015}
\citation{Schmidhuber1989}
\citation{Hochreiter1997LongMemory}
\citation{Qin}
\citation{Zhu2017DeepUber}
\citation{Tang2017RethinkingDemands}
\citation{Jiang2017Pytheas:Exploration-Exploitation}
\citation{Mao2017NeuralPensieve}
\citation{Zhao2017LearningNetworks}
\citation{Yadwadkar2017SelectingClouds}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces lstm auto-encoder with a deep feadfoward network}}{7}{figure.8}}
\newlabel{fig:our_models}{{8}{7}{lstm auto-encoder with a deep feadfoward network}{figure.8}{}}
\@LN@col{1}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Baseline}{7}{subsection.5.2}}
\@LN@col{2}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Work}{7}{section.6}}
\citation{Krishnan2000}
\citation{Hasan2014}
\citation{Tang2017RethinkingDemands}
\citation{Borst2010}
\citation{Leconte2016}
\citation{Applegate2016}
\citation{Jiang2017Pytheas:Exploration-Exploitation}
\citation{JiangCFA:Optimization}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Performance Comparison}}{8}{table.2}}
\newlabel{my-label}{{2}{8}{Performance Comparison}{table.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Prediction Results of LSTM autoencoder with feed-forward neural network}}{8}{figure.9}}
\newlabel{fig:Prediction_Results}{{9}{8}{Prediction Results of LSTM autoencoder with feed-forward neural network}{figure.9}{}}
\@LN@col{1}
\@LN@col{2}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion and Future Work}{8}{section.7}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Acknowledgment}{8}{section.8}}
\bibstyle{unsrt}
\bibdata{mendeley}
\bibcite{Jiang2017Pytheas:Exploration-Exploitation}{{1}{}{{}}{{}}}
\bibcite{Mao2017NeuralPensieve}{{2}{}{{}}{{}}}
\bibcite{Li2017}{{3}{}{{}}{{}}}
\bibcite{JiangVIA:Selection}{{4}{}{{}}{{}}}
\bibcite{GaoMachineOptimization}{{5}{}{{}}{{}}}
\bibcite{Lecun2015}{{6}{}{{}}{{}}}
\bibcite{HermansTrainingNetworks}{{7}{}{{}}{{}}}
\bibcite{Bengio1994LearningDifficult}{{8}{}{{}}{{}}}
\bibcite{ChoLearningTranslation}{{9}{}{{}}{{}}}
\bibcite{Hochreiter1997LongMemory}{{10}{}{{}}{{}}}
\bibcite{MalhotraLongSeries}{{11}{}{{}}{{}}}
\bibcite{Schmidhuber1989}{{12}{}{{}}{{}}}
\bibcite{Qin}{{13}{}{{}}{{}}}
\bibcite{Zhu2017DeepUber}{{14}{}{{}}{{}}}
\bibcite{Tang2017RethinkingDemands}{{15}{}{{}}{{}}}
\bibcite{Zhao2017LearningNetworks}{{16}{}{{}}{{}}}
\bibcite{Yadwadkar2017SelectingClouds}{{17}{}{{}}{{}}}
\bibcite{Krishnan2000}{{18}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The RMSE for the different forecast models}}{9}{figure.10}}
\newlabel{fig:errors_comparaison_of_different_models}{{10}{9}{The RMSE for the different forecast models}{figure.10}{}}
\@LN@col{1}
\@LN@col{2}
\bibcite{Hasan2014}{{19}{}{{}}{{}}}
\bibcite{Borst2010}{{20}{}{{}}{{}}}
\bibcite{Leconte2016}{{21}{}{{}}{{}}}
\bibcite{Applegate2016}{{22}{}{{}}{{}}}
\bibcite{JiangCFA:Optimization}{{23}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@LN@col{1}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Feature Aggregation and Selection}}{10}{algorithm.1}}
\@LN@col{2}

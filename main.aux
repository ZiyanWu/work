\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{elsarticle-num}
\emailauthor{lzh@fudan.edu.cn}{Zhihui Lu\corref {mycorrespondingauthor}}
\citation{Jiang2017Pytheas:Exploration-Exploitation}
\citation{Mao2017NeuralPensieve}
\citation{Li2017}
\citation{JiangVIA:Selection}
\citation{GaoMachineOptimization}
\citation{Edge}
\Newlabel{mymainaddress}{a}
\Newlabel{hismainaddress}{b}
\Newlabel{hisaddress}{c}
\@LN@col{1}
\Newlabel{mycorrespondingauthor}{1}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@LN@col{2}
\citation{Lecun2015}
\citation{Langkvist2014AModeling}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.2}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The basic working procedure of CDN}}{2}{figure.1}}
\newlabel{fig:CDN}{{1}{2}{The basic working procedure of CDN}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The structure of cache server groups}}{2}{figure.2}}
\newlabel{fig: the structure of PoP}{{2}{2}{The structure of cache server groups}{figure.2}{}}
\@LN@col{1}
\@writefile{toc}{\contentsline {section}{\numberline {3}Problem Formulation and Models}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Linear Models for Sequence Learning}{3}{subsection.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces list of candidate input features from one cache server group. We organize the features into groups. The first group is the raw data we directly collected from the caching servers. The second group are the features we construct based on statistics of the metrics of a single cache server. The third group are the features we construct based on the statistics of the metrics of the whole cache server groups}}{3}{table.1}}
\newlabel{my-label}{{1}{3}{list of candidate input features from one cache server group. We organize the features into groups. The first group is the raw data we directly collected from the caching servers. The second group are the features we construct based on statistics of the metrics of a single cache server. The third group are the features we construct based on the statistics of the metrics of the whole cache server groups}{table.1}{}}
\@LN@col{1}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Non-linear for Sequence Learning}{4}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Feed Forward Neural Network}{4}{subsubsection.3.2.1}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces MLP structure}}{4}{figure.3}}
\newlabel{fig:RNN}{{3}{4}{MLP structure}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Sigmoid function as activation function}}{4}{figure.4}}
\newlabel{fig:sigmoid}{{4}{4}{Sigmoid function as activation function}{figure.4}{}}
\citation{HermansTrainingNetworks}
\citation{Bengio1994LearningDifficult}
\citation{ChoLearningTranslation}
\citation{Bengio1994LearningDifficult}
\citation{Hochreiter1997LongMemory}
\@LN@col{1}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}RNN}{5}{subsubsection.3.2.2}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The left shows the backloop structure of RNN and right shows that RNN can be thought as infinite deep layers neural network unfolded along the dimension of time}}{5}{figure.5}}
\newlabel{fig:RNN can be thought as infinite deep layers neural network along the dimensions of time}{{5}{5}{The left shows the backloop structure of RNN and right shows that RNN can be thought as infinite deep layers neural network unfolded along the dimension of time}{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Methods}{5}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Feature Engineering}{5}{subsection.4.1}}
\citation{Hochreiter1997LongMemory}
\citation{MalhotraLongSeries}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces three steps of feature enginnering}}{6}{figure.6}}
\newlabel{fig:Correlation_matrix}{{6}{6}{three steps of feature enginnering}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Correlation Matrix of the feature set}}{6}{figure.7}}
\newlabel{fig:Correlation_matrix}{{7}{6}{Correlation Matrix of the feature set}{figure.7}{}}
\@LN@col{2}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Feature Clustering and Selection}}{6}{algorithm.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Prediction Model Design}{6}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}LSTM}{6}{subsubsection.4.2.1}}
\citation{TensorFlow}
\@LN@col{1}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces LSTM cell}}{7}{figure.8}}
\newlabel{fig:LSTMCELL}{{8}{7}{LSTM cell}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces tanh as activation function}}{7}{figure.9}}
\newlabel{fig:tanh}{{9}{7}{tanh as activation function}{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}LSTM auto-encoder}{7}{subsubsection.4.2.2}}
\@LN@col{2}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces LSTM Auto-encoder Model}}{7}{figure.10}}
\newlabel{fig:RNN_encoder-decoder}{{10}{7}{LSTM Auto-encoder Model}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Evaluation}{7}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Experimental Settings and Dataset}{7}{subsection.5.1}}
\citation{Lecun2015}
\citation{Schmidhuber1989}
\citation{Hochreiter1997LongMemory}
\citation{Qin}
\citation{Zhu2017DeepUber}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces lstm auto-encoder with a deep feadfoward network}}{8}{figure.11}}
\newlabel{fig:our_models}{{11}{8}{lstm auto-encoder with a deep feadfoward network}{figure.11}{}}
\@LN@col{1}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Baseline}{8}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Experimental Results}{8}{subsection.5.3}}
\@LN@col{2}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related Work}{8}{section.6}}
\citation{Tang2017RethinkingDemands}
\citation{Jiang2017Pytheas:Exploration-Exploitation}
\citation{Mao2017NeuralPensieve}
\citation{Zhao2017LearningNetworks}
\citation{Yadwadkar2017SelectingClouds}
\citation{Krishnan2000}
\citation{Hasan2014}
\citation{Tang2017RethinkingDemands}
\citation{Borst2010}
\citation{Leconte2016}
\citation{Applegate2016}
\citation{Jiang2017Pytheas:Exploration-Exploitation}
\citation{JiangCFA:Optimization}
\bibstyle{unsrt}
\bibdata{mendeley}
\bibcite{Jiang2017Pytheas:Exploration-Exploitation}{{1}{}{{}}{{}}}
\bibcite{Mao2017NeuralPensieve}{{2}{}{{}}{{}}}
\bibcite{Li2017}{{3}{}{{}}{{}}}
\bibcite{JiangVIA:Selection}{{4}{}{{}}{{}}}
\bibcite{GaoMachineOptimization}{{5}{}{{}}{{}}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Performance Comparison}}{9}{table.2}}
\newlabel{my-label}{{2}{9}{Performance Comparison}{table.2}{}}
\@LN@col{1}
\@LN@col{2}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion and Future Work}{9}{section.7}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Acknowledgment}{9}{section.8}}
\bibcite{Edge}{{6}{}{{}}{{}}}
\bibcite{Lecun2015}{{7}{}{{}}{{}}}
\bibcite{Langkvist2014AModeling}{{8}{}{{}}{{}}}
\bibcite{HermansTrainingNetworks}{{9}{}{{}}{{}}}
\bibcite{Bengio1994LearningDifficult}{{10}{}{{}}{{}}}
\bibcite{ChoLearningTranslation}{{11}{}{{}}{{}}}
\bibcite{Hochreiter1997LongMemory}{{12}{}{{}}{{}}}
\bibcite{MalhotraLongSeries}{{13}{}{{}}{{}}}
\bibcite{TensorFlow}{{14}{}{{}}{{}}}
\bibcite{Schmidhuber1989}{{15}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Prediction Results of LSTM auto-encoder}}{10}{figure.12}}
\newlabel{fig:prediction_Results_of_LSTM_auto_encoder}{{12}{10}{Prediction Results of LSTM auto-encoder}{figure.12}{}}
\@LN@col{1}
\@LN@col{2}
\bibcite{Qin}{{16}{}{{}}{{}}}
\bibcite{Zhu2017DeepUber}{{17}{}{{}}{{}}}
\bibcite{Tang2017RethinkingDemands}{{18}{}{{}}{{}}}
\bibcite{Zhao2017LearningNetworks}{{19}{}{{}}{{}}}
\bibcite{Yadwadkar2017SelectingClouds}{{20}{}{{}}{{}}}
\bibcite{Krishnan2000}{{21}{}{{}}{{}}}
\bibcite{Hasan2014}{{22}{}{{}}{{}}}
\bibcite{Borst2010}{{23}{}{{}}{{}}}
\bibcite{Leconte2016}{{24}{}{{}}{{}}}
\bibcite{Applegate2016}{{25}{}{{}}{{}}}
\bibcite{JiangCFA:Optimization}{{26}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Prediction Results of LSTM auto-encoder with feed-forward neural network (downsampled)}}{11}{figure.13}}
\newlabel{fig:prediction_results}{{13}{11}{Prediction Results of LSTM auto-encoder with feed-forward neural network (downsampled)}{figure.13}{}}
\@LN@col{1}
\@LN@col{2}

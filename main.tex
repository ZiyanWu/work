\documentclass[5p]{elsarticle}
\usepackage{graphicx}
\graphicspath{ {pic/} }
\usepackage{lineno,hyperref}
\usepackage{amsmath}
\usepackage{natbib}

%https://www.cnblogs.com/coolqiyu/p/5580290.html
\usepackage[noend]{algpseudocode}
\usepackage{algorithmicx,algorithm}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amsmath}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm  
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm  
\modulolinenumbers[5]

\journal{Journal of Parallel and Distributed Computing}

\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\dabiaolv}{reach rate }


\begin{document}

\begin{frontmatter}

\title{A Data-driven Approach of Performance Evaluation for Cache Server Groups in Content Delivery Network
    %\tnoteref{mytitlenote}
    %JPDC-A data-driven framework for performance evaluation for CDN cache groups
    }
%\tnotetext[mytitlenote]{Fully documented templates are available in the elsarticle package on %\href{http://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle}{CTAN}.}

%% Group authors per affiliation:
\author[mymainaddress]{Ziyan Wu\corref{ziyanwuemail}
    }

\author[mymainaddress]{Zhihui Lu\corref{mycorrespondingauthor}}

\cortext[mycorrespondingauthor]{Corresponding author}
\ead{lzh@fudan.edu.cn}
\author[mymainaddress]{Wei Zhang
    }
\author[mymainaddress]{Jie Wu
    }
\author[hismainaddress]{Shalin Huang}
\author[hisaddress]{Patrick C. K. Hung}


\address[mymainaddress]{School of Computer Science, Fudan University, Shanghai 200433, China}
\address[hismainaddress]{Wangsu Science & Technology Co., Ltd., Shanghai}
\address[hisaddress]{Faculty of Business and IT，University of Ontario Institute of Technology, Canada}

%\fntext[myfootnote]{Since 1880.}

\begin{abstract}
In industry, Content Delivery Network(CDN) Service providers are increasingly using data-driven mechanisms to build their performance models of their service-providing systems. To build a model to accurately describe the performance of the existing infrastructure is very crucial to make resource management decisions. Conventional approaches that use hand-tuned parameters or linear models have their drawbacks. Recently, data-driven paradigm have been shown to greatly outperform traditional methods in modeling complex systems. We design a approach that using these techniques to build a performance model for CDN cache server groups. We use deep LSTM auto-encoder to capture the temporal structures from the high-dimensional monitoring data, and use a deep neural network to predict the \dabiaolv, which is a client QoS measurement from the CDN service providers perspective. The experimental results have shown that our model is able to outperform state-of-the-arts models.
\end{abstract}
\begin{keyword}
\textit{edge computing, deep learning, content delivery networks, sequence learning, predictive analysis, high dimensional data}
%\texttt{elsarticle.cls}\sep \LaTeX\sep Elsevier \sep template
%\MSC[2010] 00-01\sep  99-00
\end{keyword}
\end{frontmatter}
\linenumbers
\section{Introduction}
% the detail of abstract
There is a trend \cite{Jiang2017Pytheas:Exploration-Exploitation} \cite{Mao2017NeuralPensieve} \cite{Li2017} \cite{JiangVIA:Selection} \cite{GaoMachineOptimization} that using data-driven methods to model complex networked systems. Traditional approaches typically use some simple heuristics. These methods have several drawbacks. They cannot accurately reflect the complex systems due to the lack of knowledge of real-word environment. Driven by the opportunity to collect and analyze data (e.g., application quality measurement from end users), many recent proposals have demonstrated the promise of using deep learning to characterize and optimize networked systems. Drawing parallel from the success of deep-learning on pattern recognizaition, instead of using empirical analytical model to describe the complex interaction of different features, we use deep learning methods and treat networked systems as a black-box.

Uploading all data  or deploying all applications to a centralized cloud  is infeasible because of the excessive latency and bandwidth limitation of the Internet. A promising approach to addressing centralized cloud bottleneck is edge computing. Edge computing pushes applications, data and computing power (services) away from centralized points to the logical extremes of a network. Edge computing replicates fragments of information across distributed networks of web servers, which may spread over a vast area. As a technological paradigm, edge computing is also referred to as mesh computing, peer-to-peer computing, autonomic (self-healing) computing, grid computing, and by other names implying non-centralized, nodeless availability\cite{Edge}. CDN ( content delivery network or content distribution network) is a typical representative of edge computing. A CDN is a globally distributed networked system deployed across the edge of Internet. Composed with geographically distributed cache servers, CDNs deliver cached content  to  customers  worldwide  based  on their geographic locations. Extensively  using  cache  servers,  content  delivery over  CDN  has  low  latency, reliability, supports better quality of experience and security.

The CDN Service providers are increasingly using data-driven mechanisms to build their performance model of their service-providing systems. To build a model to accurately provide an understanding of the performance of the existing infrastructure such as the health of cache groups and network status, is very crucial to make resource management decisions including content placement, network traffic scheduling, load balance of the CDN network. Modeling all available physical resources, we can maximize a resource utilization in terms of service quality, cost, profit, etc.

Generally CDN providers don't have direct measurement from the clients (the logs from video players, web browser that can show the QoE of clients). Instead, they use the indirect measurement \dabiaolv which is collected and calculated from the log of the HA proxy of CDN cache groups. The computation of \dabiaolv is done offline . In order to enable themselves make resource management decisions in real time, the CDN providers have to use the metrics that can be collected in the real time to infer the \dabiaolv. 

Cache server groups can be characterised as multi-dimensional, highly non-linear, time variant vectors. The metrics that collected from members of the CDN cache server groups are sequence data that are measured every minute, which have hundreds of dimensions. The state-of-art methods are typically using simple heuristics  which are oversimplifed and biased due to the human experience, or linear models, which cannot characterize the complex relationship between multiple metrics. Deep learning is a branch of machine learning based on a set of algorithms that attempts to model high-level abstractions in data by using artificial neural network architectures composed of multiple non-linear transformations \cite{Lecun2015}. They have a lot of successful applications in sequence modeling\cite{Langkvist2014AModeling}. Compared to other machine learning techniques, a lot of work show that it can detect complex relationships among features, can extract hierarchical level of features from high-dimensional data, including monitoring data.

%Resouoce management involves 
We frame our problem as a sequence learning problem, which consists of stages: (1) feature engineering (2) representation learning by lstm auto-encoder to extract useful (3) a feed foward neural network black-box machine learning algorithm to output the predictions. 
%\deep-

Our main contributions are listed below:
\begin{itemize}
  \item We frame performance evaluation problem as a sequence learning problem.
  \item We use representation learning by lstm auto-encoder to extract useful features from data.
  \item We compare our methods with state-of-arts methods and show ours is superior by empirical studies.
\end{itemize}

The remain organization of this paper is as follows. In Section 2, we first introduce the related concept as our research background. In Section 3, we formulate our performance evaluation problem as a sequence learning problem and then compare the baseline methods. In Section 4, we introduce our method of feature engineering to reduce the dimensionality for the high-dimensional data and  our \dabiaolv prediction model based on lstm auto-encoder. In Section 5, we show our experiment setting and demonstrate performance improvements of our methods over baseline models. Section 6 is discussion and future work. We provide concluding remarks in Section 7.
%\cite

\section{Background}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{CDN_new.png}
    \caption{The basic working procedure of CDN}
    \label{fig:CDN}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{cache_group.png}
    \caption{The structure of cache server groups}
    \label{fig: the structure of PoP}
\end{figure}

A content delivery network or content distribution network (CDN) (figure \ref{fig:CDN}) is a geographically distributed network of cache servers. CDN helps content provider deliver web pages and other multimedia content to the clients, based on the locations of the clients and cache servers nearby the clients. The basic working procedure is as follows. Step 1: client sends a request to local DNS. Step 2: the DNS finds the CNAME and redirects the request to the GSLB (global sever load balance). Step 3: the local DNS server sends a request to the GSLB and GSLB returns the ip address of CDN servers based on the scheduling policy. Step 4: the local DNS return the ip address to the clients. Step 5: clients request to fetch content from the selected PoP. Step 6: the cache server groups will pull the content from source website if the content doesn't exist locally. Step 7: content is sent to the clients.

A CDN cache server group (figure \ref{fig: the structure of PoP}) is the basic resource scheduling unit for CDN. A CDN cache group is a load balanced cluster that consists of interconnected cache servers. A typical implementation consists of HAproxy and squid servers. HAproxy distributes the requests from clients to cache servers. HAproxy can be set to use different algorithms to maximize the utilization of every server. Round-robin algorithm distributes the load equally to each server in a homogenous cluster. In a heterogeneous cluster, weighted round-robin algorithm is used. A weight was assigned to the server based on its processing capabilities. A heterogeneous structure of a cluster adds complexity to the feature engineering.

As CDN providers do not have direct QoS measurement from the clients (the logs from video players, web browsers that can show the QoE), so they use the measurement \dabiaolv. They calculated \dabiaolv from log of the HA proxy of CDN cache groups with a delay about three minutes.  

The number of metrics we collected from the cache servers of cache group are different due to the different configuration, from 64 to 134. We listed the features we constructed in Table \ref{feature_list}. There are about 312 features. They include cpu utilization, network utilization, disk utilization, memory utilization and so on.

\begin{table}[]
\centering
\begin{tabular}{|c|}
\hline  
feature list\\
\hline  
group 1\\
\hline  
cpu1.usage\\
cpu2.usage\\
cpu3.usage\\
cpu4.usage\\
cpu5.usage\\
cpu6.usage\\
...  \\
mem\_cached\\
mem\_buffers\_cache\_free\\
memory.swap\\ 
disk.used.sda1\\
disk.used.sda2\\
disk.used.sda3\\
disk.used.sda4\\
...\\
channeltraffic\_in\\
channeltraffic\_in\\
ioutil\_util\_sda\\
ioutil\_util\_sdb\\
ioutil\_util\_sdc\\
ioutil\_util\_sdd\\
...\\
iowait.wait\\
hitratio.port8101\\
hitratio.port8102\\
...\\
load.total_load\\
resptime.resp.resp8105\\
resptime.resp.resp80\\
resptime.resp.resp8101\\
...\\
\hline  
group 2\\
\hline 
aggregate.cpu\\
aggregate.diskused\\
aggregated.ioutil\\
aggregate.CPU.max\\
aggregate.resptime\\
....\\
\hline  
group 3\\
\hline 
aggregate.all\_machine.cpu\\
aggregate.all\_machine.diskused\\
aggregated.all\_machine.ioutil\\
aggregate.all\_machine.CPU.max\\
aggregate.all.resptime\\
...\\
\hline
\end{tabular}
\caption{List of candidate input features from one cache server group. We organize the features into groups. The features in the first group from the raw data we directly collected from the caching servers. The features of the second group are what we construct based on statistics of the metrics of a single cache server. The features of third group are what we construct based on the statistics of the metrics of the whole cache server groups}
\label{feature_list}
\end{table}

\section{Problem Formulation and Models}
We argue that performance evaluation as a sequence learning problem. Since we can collect the caching servers performance metrics and network metrics at time intervals of one minute, we can use a sequence models to describe the relationship between metrics collected from cache groups and \dabiaolv.

There are three categories of sequence learning problem, which are many to many, one to many and many to one. Our goal is to model the relationship between a sequence collected metrics and \dabiaolv within a certain period of time, which is many to one. In general, we can use the following formulation to describe the prediction process.

Giving a sequence of vectors, $\{x_n\}=\{x_{\alpha} \in R^{d}|\alpha \in N\}$, where $d$ is the number of the features, we use $\{x_n\}$ to represent the sequence and $x_t$ to describe a data point at time t with $d$ dimensions.

Given another target sequence, $\{y_n\}=\{y_{\alpha} \in R|\alpha \in N\}$, our goal is to find the relation between$\{x_n\}$ and $\{y_n\}$, which is 
$$y_t=f(x_{t},x_{t-1},...,x_{t-n+1})$$
where $n$ is the window size and $f$ is the mapping we want to learn from the data.

Many models can be used to approximate $f$ in sequence modeling. The most naive way is to use simple heuristics, which is use an exponential moving average to linearly map each metrics to a score in a certain time interval. The parameters are depending on the experience of the operators. This method is impractical: it can hardly generalize well and requires tedious repetitive tuning.

\subsection{Linear Models for Sequence Learning}

There are some conventional approaches which use data to learn such as using the multiple linear regression model (MLR). The MLR method builds a model of a sequence that is composed of a linear part and a random noise part. The linear part models the linear relationship between the dependent variables and independent variables, the random noise reflects the unpredictable randomness. Formally, the model for multiple linear regression, given n observations, is 
    $$y_t = c+\sum_{i=t-n+1}^{t}(\beta_i ∗ x_{t_i})+\epsilon_t$$

Furthermore, the linear part incorporates historical values of the sequence. $y$ represents the target we want to model. $c$ represents the constant parameter of the linear decomposition, $\beta$ represents the parameters to be computed and epsilon reflects the random noise part. The best-fitting line for the observed data is calculated by the least square method.

Linear models are easy to implement and have good interpretation and thus are widely used in much real works. However, linear models are shown not sufficient to describe some nonlinear behaviors of the complex network systems. In many cases, neural networks tend to outperform linear models. In our experiments, we observe a non-linear relationship between the metrics and \dabiaolv.

\subsection{Non-linear for Sequence Learning}

Deep learning has many applications in sequence learning. Deep learning is a branch of machine learning based on a set of algorithms that attempts to model high-level abstractions in data by using artificial neural network (ANN) architectures composed of multiple non-linear transformations. They have a lot of successful applications in sequence modeling. Compared to other machine learning techniques, it can detect complex relationships among features, can extract a hierarchical level of features from raw data. So it can build a model more accurate with less time. They are advantageous for modeling intricate systems because neural networks do not require the user to predefine the feature interactions in the model, which assumes relationships within the data. Instead, the neural network searches for patterns and interactions between features to automatically generate the best fit model.


\subsubsection{Feed Forward Neural Network}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{mlp.png}
    \caption{MLP structure}
    \label{fig:RNN}
\end{figure}

A generic three-layered neural network is illustrated in Figure 2. In this study, the input matrix $I \in R^{M\times(N\timesT+1)}$  where M is the number of training examples and $N\timesT+1$ is the number of features (metrics that collected from the cache servers) concatenating the bias term. The input matrix is then multiplied by the model parameters matrix $W_1$ to produce the hidden state matrix $h$. The output of the first layer is transformed by an activation function. We can add more layers to the network. The size and number of hidden layers can be varied to model systems of varying complexity. The whole process can be imagined as the information propagating forward. We can formalize the forward propagation as follows:

\begin{equation}
    \begin{split}
    & \mathbf h_1 = \text{sigmoid}(\mathbf W_1 * \mathbf I) \\
    & \mathbf h_t = \text{sigmoid}(\mathbf W_{t-1} * \mathbf h_{t-1}) \\
    & \mathbf O=\text{sigmoid}(\mathbf W * \mathbf h_{lastlayer}) \\
    \end{split}
\end{equation}

where sigmoid is the activation function:
\begin{equation}
    \begin{split}
    & \text{sigmoid}(x)=\frac{1}{1+e^{-x}}
    \end{split}
\end{equation}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{sigmoid.png}
    \caption{Sigmoid function as activation function}
    \label{fig:sigmoid}
\end{figure}

the loss function $J$ uses mean square error:
    $$J=\frac{1}{n}\times{\sum_{t=1}^N(y^{'}{_t}-y_t})^2$$

The process of training any neural network model can be broken down into four steps: (1) Randomly initialize the model parameters, (2) forward propagation algorithm,(3) Compute the cost function $J$, (4) back propagate using the chain rule  (5) using the gradients calculated, adjust weights to minimize the cost function $J$.

One major assumption for NNs is the independence of samples. For sequence learning problem, however, this assumption doesn't hold true, for the samples of our problem have a temporal relationship: the status of the system of the next timestep not only depends on the status in the current timestep but also on the previous timesteps of indefinite length. One solution is to use a sliding window to capture the sequential relationship between the samples. The performance of this method depends on the window size, which isn't practical for the dependencies length which isn't a fixed value. RNN eliminates the need to find the size of the window\cite{HermansTrainingNetworks}. 

\subsubsection{RNN}

RNNs are a class of neural networks that depend on the sequential nature of their inputs. Such inputs could be text, speech, time series, and anything else in which the occurrence of an element in the sequence is dependent on the elements that appeared before it. The promise of recurrent neural networks is that the temporal dependence and contextual information in the input data can be learned\cite{Bengio1994LearningDifficult} \cite{ChoLearningTranslation}. 

RNNs \ref{fig:RNN} process the input sequence one element at a time and maintain a hidden
state vector which acts as a memory for past information. They learn to selectively retain relevant information allowing them to capture dependencies across several time steps. This allows them to utilize both current input and past information while making future predictions. All this is learned by the model automatically without much knowledge of the cycles or time dependencies in data. RNNs obviate the need for a fixed size time window and can also handle variable length sequences. Moreover, the number of states that can be represented by an NN is exponential in the number of nodes.

 
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{RNN.png}
    \caption{The left shows the backloop structure of RNN and right shows that RNN can be thought as infinite deep layers neural network unfolded along the dimension of time}
    \label{fig:RNN can be thought as infinite deep layers neural network along the dimensions of time}
\end{figure}


RNNs maintain a hidden vector $\mathbf h$, which is updated at time step $t$ as follows:

\begin{equation}
	\mathbf h_t = \tanh(\mathbf W * \mathbf h=s_{t-1} + \mathbf I * x_t)
\end{equation}

where $\tanh$ is the hyperbolic tangent function, $\mathbf W$ is the recurrent weight matrix and $I$ is a input wight matrix. The hidden state $\mathbf h$ is then used to make a prediction

\begin{equation}
	\mathbf y_t = f(\mathbf W' * \mathbf h_{t})
\end{equation}

where $f$ can be fully connected layer that linearly maps the hidden state to an output. By using $\mathbf h$ as the input to another RNN, we can stack RNNs, creating deeper architectures 

\begin{equation}
	\mathbf h_t^{l} = \sigma(\mathbf W * \mathbf h_{t-1}^{l} + \mathbf I * \mathbf h_t^{l-1}).
\end{equation}

The training of RNNs uses back-propagation through time (BPTT). RNNs have several drawbacks: 1. Training vanilla RNNs is known to be particularly difficult, with vanishing and exploding gradients being one possible explanation \cite{Bengio1994LearningDifficult}. 2. RNNs aren't capable of learning long-term dependencies. LSTM address these issues by introducing LSTM cell\cite{Hochreiter1997LongMemory} 


\section{Methods}
\subsection{Feature Engineering}
The feature engineering is the process after data-cleansing, in which we fill the missing data and reformat it. The propose of this stage is two-fold:
(1) to find a unified equal-length vector representation of all of the cache groups. 
The metrics collected are in the granularity of caching servers which have different dimensionality. As showed in graph. To make things more complex, a cache group may have different number caching servers. As we only care about. 
(2) reduce the dimensionality, for it is very time-consuming to train the models when the number of dimensionality is too high.

Our feature engineering has three steps: 1. data prepossessing which converts raw data into high-dimensional vectors. 2. feature correlation analysis that characterizes the linear relationship between every pair of features. 3. cluster analysis that selects the set of features.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{feature_engineering.png}
    \caption{three steps of feature enginnering}
    \label{fig:Correlation_matrix}
\end{figure}

As there are hundreds of features, there are many overlaps among the variables. We use correlation in statistics to group highly correlated variables and create composite features that can represent each group of variables.

Correlation is an analysis of two or more observed or random variables to determine a dependence between the variables. This dependence can be classified as the probability that changes in one variable affect the behavior of the second variable. The Pearson’s correlation defines this dependence in the interval between -1 and 1. Pearson’s correlation for two given random variables X and Y are computed by dividing the covariance of both variables with the product of their standard deviations.

Generally, cases of high correlation compute to a value close to 1.0, high anticorrelation is associated with a value close to -1.0 and no correlation is assumed. if the value is around 0.0, the variables appear to be linearly independent. 

The correlation matrix we calculate is pictured in figure \ref{fig:Correlation_matrix}. The white color denotes the correlation of 1.0 and the black denotes -1.0. The red denotes that there is no linear relationship at all. As we can see from the picture, there are overlaps among the features.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Correlation_Matrix.png}
    \caption{Correlation Matrix of the feature set}
    \label{fig:Correlation_matrix}
\end{figure}

After we calculate the correlation matrix for all the features, the correlations of these features are regarded as distance. Then, we use DBSCAN to cluster these features to eliminate close ones. By defining the neighborhood parameter $\epsilon$, we specify how much extent we regard two features as close. If a feature is highly correlated with another one, we should add these features into the same cluster. When the clustering is done, we choose the representative feature from the cluster. We formulate the process in Algorithm 1.

\begin{algorithm}[t]
\caption{Feature Clustering and Selection} 
\begin{algorithmic}[1]
\Require  
    All features list: $F$;
    Neighborhood parameter $\epsilon$; 
\Ensure  
    Selected features list： $F_S$;  
\State Initialize the set of core object:  $\Omega$=$\emptyset$;
\For{$i \;in\; F$}
    \State Confirm the $\epsilon$-neighborhood\; N(i,$\epsilon$);
    \If {$|N(i,\epsilon)|$\textgreater 1} % If 语句，需要和EndIf对应
　　　　        \State Add i to the core object set: $\Omega$;
　　        \Else
　　　　        \State Add i to the selected features list： $F_S$;
　　　　    \EndIf
\EndFor
\State Initialize the number of clusters:  $k$=0;
\State Initialize the set of features:  $\Gamma$= $\Omega$;
\While{$\Omega\neq\emptyset$}
\State Check the set of unvisited features: $\psi$= $\Gamma$;
    \State Random select an core object $o$ $\in$ $\Omega$,initilaize the queue $Q$=$\langle o \rangle$;
\State $\Gamma$=$\Gamma \verb|\|\left\{o\right\}$;
\While{$Q\neq\emptyset$}
    \State Fetch an element from Q: $q$;
    \State $\Delta=N(q,\epsilon)\bigcap\Gamma$;
    \State Add the elements in $\Delta$ to Q;
    \State  $\Gamma= \Gamma\verb|\|\Delta$;
\EndWhile
\State $k=k+1$;
\State Generate the cluster C$_k$=$\psi\verb|\|\Gamma$;
\State $\Omega=\Omega\verb|\|C_k$;
\EndWhile
\For{i=1,2...k}
    \State Fetch an element in $C_k$ out to the selected features list $F_s$
\EndFor
\State \Return $F_s$
\end{algorithmic}
\end{algorithm}


\subsection{Prediction Model Design}

In this section, we introduce how the architecture we use to predict \dabiaolv of CDN cache group using the data output from the feature engineering stage. The key components of our model are LSTM, LSTM auto-encoder, and deep feed foward neural network.

\subsubsection{LSTM}
LSTM, introduced in \cite{Hochreiter1997LongMemory}, addresses the problem of vanishing gradients by introducing a memory cell. \cite{MalhotraLongSeries} applied in time series. The inner working of LSTM(figure \ref{fig:LSTMCELL}) are listed follows:

\begin{equation}
	\begin{split}
		& \mathbf g_u = \sigma(\mathbf W_u * \mathbf h_{t-1} + \mathbf I_u * x_t) \\
		& \mathbf g_f = \sigma(\mathbf W_f * \mathbf h_{t-1} + \mathbf I_f * x_t) \\
		& \mathbf g_o = \sigma(\mathbf W_o * \mathbf h_{t-1} + \mathbf I_o * x_t) \\
		& \mathbf g_c = \tanh(\mathbf W_c * \mathbf h_{t-1} + \mathbf I_c *  x_t) \\
		& \mathbf m_t = \mathbf g_f \odot \mathbf +  \mathbf g_u \odot \mathbf g_c \\
		& \mathbf h_t = \tanh(\mathbf g_o \odot \mathbf m_{t-1}) 
	\end{split}
\end{equation}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{lstm_cell.png}
    \caption{LSTM cell}
    \label{fig:LSTMCELL}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{tanh.png}
    \caption{tanh as activation function}
    \label{fig:tanh}
\end{figure}

here $\sigma$ is the logistic sigmoid function, $\mathbf W_u, \mathbf W_f, \mathbf W_o, \mathbf W_c$ are recurrent weight matrices and $\mathbf I_u, \mathbf I_f, \mathbf I_o, \mathbf I_c$ are projection matrices.


\subsubsection{LSTM auto-encoder}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{auto_encoder.png}
    \caption{LSTM Auto-encoder Model}
    \label{fig:RNN_encoder-decoder}
\end{figure}

An LSTM auto-encoder (figure \ref{fig:RNN_encoder-decoder}) contains: an encoding function, a decoding function, and a distance function between the amount of information loss between the compressed representation of your data and the decompressed representation (i.e. a "loss" function). The encoder and decoder will be chosen to be deep layered lstm network. So the parameters of the encoding/decoding functions can be optimize to minimize the reconstruction loss, using Stochastic Gradient Descent.

Prior to fitting the data prediction model, we first conduct a pre-training step to fit an encoder that can extract useful and representative embeddings froma  time  series.  The  goals  are  to  ensure  that  (i)  the  learned embedding  provides  useful  features  for  prediction  and  (ii) unusual input can be captured in the embedded space, which will get further propagated to the prediction network in the next step.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{our_models.png}
    \caption{lstm auto-encoder with a deep feadfoward network}
    \label{fig:our_models}
\end{figure*}

As illustrated in figure \ref{fig:our_models}. Given a multivariate time series of data, the encoder reads the vectors as input and transform them as latent variables. During the pretraining, only the weights of LSTM auto-encoder are trained. The LSTM auto-encoder is trained to reconstruct the input on the output side. 

In the second step, a fully connected feed foward network take the latent variables which are the outputs of the encoder as input. We take the latent variables as training set and \dabiaolv as the targets and use the gradient descent to train the network. We use 9/10 of the data as training set and 1/10 of the data as test set.

\section{Evaluation}
\subsection{Experimental Settings and Dataset}
Our implementation uses the Google opensource deep learning library, Tensorflow\cite{TensorFlow}, version 1.2.0. We ran our experiments on a physical machine running an Ubuntu 16.04 operation system, intel i7-6700HQ, 16 GB memory, and GPU gtx1060.

To test the performance, we select two cache groups with average request above 7000 per minute. The first cache servers groups have 13 cache servers while the other has 10 cache servers. The metrics are raw data collected from the cache servers, including CPU utilization, network utilization, disk utilization, memory utilization of two cache group servers. The frequency of the metrics and \dabiaolv are both minute-by-minute. This data covers the period from December 20, 2017 to January 26, 2018.

In the experiments, 5-fold cross validation is used to evaluate the accuracy of predictions. This divides the entire data set into 5 equal parts.  Prediction is repeated 5 times, each time keeping one of the 1 parts as test data and the other 4 parts as training data to build the prediction model. Finally, test results on all the 5 parts are accumulated together to calculate the average prediction accuracy.

In the pre-training procedure We use minibatch stochcasti gradient descent (SGD) together with the Adam optimizer to train the LSTM auto-encoder model. The size of the minibatch is 128. The weights can be learned by standard lstm learning algorithm propagation throught with mean squared error as the objective function. In the training procesure we use a 4 layer feed-foward neural network. We use the batch gradient descent training algorithm to train the neural network.

\subsection{Baseline}
We compare our model with other baseline model which are listed follow:
\begin{enumerate}
  \item multiple linear regression model
  \item feed-foward neural network
  \item vanilla LSTM model
  \item LSTM encoder-decoder with a feed-foward neural network
\end{enumerate}

\subsection{Experimental Results}

We use two different evaluation metrics to compare our models and methods. Root mean squared error (RMSE) and mean absolute error (MAE) are two scale-dependent measures. Specifically, assuming $y^'$ is the target at time t and $y$ is the predicted value at time t. 


\begin{equation}
    RMSE(y^{'},y)=\sqrt{\frac{1}{n}\times{\sum_{n=1}^N(y^{'}{_t}-y_t})^2}
\end{equation}

\begin{equation}
	 MAE(y^{'},y)=\frac{1}{N}\times\left| y^{'}{_t}-y_t \right|
\end{equation}


\begin{table*}[]
\centering
\caption{Performance Comparison}
\label{my-label}
\begin{tabular}{|l|l|l|}
\hline
Models & RMSE  & MAE\\
\hline
multiple linear regression model 
& $0.07841$   & $0.07734$ \\
\hline
feed-foward neural network with sliding windows 
& 042742 & 0.03870 \\
\hline
vanilla LSTM model
& 0.01880  & 0.01642 \\
\hline
LSTM auto-encoder with feed-foward neural network (latent size = 20)& 
0.01425 &  0.01334 \\
\hline
LSTM auto-encoder with feed-foward neural network (latent size = 15)& 
0.01387 &  0.01239 \\
\hline
LSTM auto-encoder with feed-foward neural network (latent size = 10)& 
0.01084 &  0.00952 \\
\hline
LSTM auto-encoder with feed-foward neural network (latent size = 5)& 
0.01282 & 0.01271 \\
\hline    
\end{tabular}
\end{table*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{lstm_auto_encoder_result.png}
    \caption{Prediction Results of LSTM auto-encoder}
    \label{fig:prediction_Results_of_LSTM_auto_encoder}
\end{figure*}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{result.png}
    \caption{Prediction Results of LSTM auto-encoder with feed-forward neural network (downsampled)}
    \label{fig:prediction_results}
\end{figure*}

The prediction results on test set are in figure \ref{fig:prediction_results}. The performance comparision is listed in Table 2. From the experiment, we observe that that methods using neural networks are superior to the one that using linear model, for linear model cannot characterize the non-linear relationship between the features and targets. The recurrent network based methods perform better than the feed foward neural network, for it contains the hidden state. Our method performs the best, for LSTM auto-encoder can extract temporal structure from the high-dimensional data into a lower dimensional dense representation. For LSTM auto-encoder, we try four different configuration regarding the length of latent variable, which are 5, 10 , 15 and 20. We observe that although with longer latent variables LSTM auto-encoder performs better when reconstructing output as illustrated in figure \ref{fig:prediction_Results_of_LSTM_auto_encoder} , it performs worse when used to predict the \dabiaolv . We believe that it is because when the latent variable size is too large, it isn't forced to learn the representation that is most related to \dabiaolv.     
 
\section{Related Work}
In recent year, deep learning has gained success in different fields, including image recognization, speech recognization, natural language understanding\cite{Lecun2015}. RNN\cite{Schmidhuber1989} outperform the traditional feed-forward neural network in modeling sequence data because its structure can characterize the temporal structure of input data by introducing hidden state. Training RNN is hard because of vanishing gradients problem; LSTM address these issues by introducing LSTM cell\cite{Hochreiter1997LongMemory}. LSTM has been widely proven successful in sequence modeling. In \cite{Qin}, a attention-based lstm outperforms traditional methods like ARIMA. In \cite{Zhu2017DeepUber}, a robust model was used to predict number of trips and do anomaly detection for Uber. 

When evaluating the complex system, the evaluation method can fall into two catagory: model-driven method, data-driven method. In model-driven method, the mathematical model characterizing the inner components of a system has to be explained explicitly. In \cite{Tang2017RethinkingDemands}, a model-driven method is used to solve optimal cache-deployment problem. However, in data-driven method, the data characterizing the behaviour of system instead of the analytical model is necessary. In \cite{Jiang2017Pytheas:Exploration-Exploitation}, CDN selection decisions of QoE optimization decision is based on the past experience of decisions and decisions outcome. In \cite{Mao2017NeuralPensieve}, a ABR decision framework using deep reinforcement learning treats network as a black box. In \cite{Zhao2017LearningNetworks}, a convolutional bi-directional LSTM network is used to predict the machine health by sensor data. in \cite{Yadwadkar2017SelectingClouds}, the VM selection mechanism allows users to balance performance gains with cost reductions. From the characteristics of the above methods, the data driven method, which takes the gathered data as basis and is independent of the object’s prior knowledge, is a more useful approach for performance evaluation, fault detection and reliability evaluation.

There are extensive research regarding CDN, for a large portion of internet traffic was boosted by service provided by CDN. There are three category: (1) long-term network planning, including optimized CDN design that relates to PoP selection and cache deployment\cite{Krishnan2000}\cite{Hasan2014}\cite{Tang2017RethinkingDemands}, and (2) short-term, run-time cache management, including content replacement and prefetching strategies in the CDN network\cite{Borst2010}\cite{Leconte2016}\cite{Applegate2016}. (3) CDN selection to optimize QoE for the client\cite{Jiang2017Pytheas:Exploration-Exploitation}\cite{JiangCFA:Optimization}. Our research falls in the second categories.

\section{Conclusion and Future Work}

In this paper we present a data-driven approach to evaluate the performance of cache server groups. The lstm auto-encoder can capture the long-term temporal information in the sequences. This paper shows that it is feasible to apply state-of-the-art deep learning techniques to model networked systems that provides estimation for its performance. The empirical studies shows that ours has outperformed the conventional methods.

There are hundreds of cache groups in China. Although our method can be used to train on all kinds of cache group of different structures, the trained model is for one specific cache group. A unified model for all kinds of cache groups is required. As the reach rate is an indirect measurement of QoS of clients, collecting data from clients ends will provide useful insights. We also want to develop the online training methods for our models because we observe that the relation between sensors and the reach rate changes over time.

\section{Acknowledgment}
The work of this paper is supported by National Natural Science Foundation of China under Grant No.61728202-Research on Internet of Things Big Data Transmission and Processing Architecture based on Cloud-Fog Hybrid Computing Model & Grant No. 61572137-Multiple Clouds based CDN as a Service Key Technology Research, and Shanghai 2016 Innovation Action Project under Grant 16DZ1100200-Data-trade-supporting Big Data Testbed.


\section*{References}
\bibliographystyle{unsrt}
\bibliography{mendeley}
\end{document}